# mini_catgirl_sft_v6_nosys.py
# ç›®æ ‡ï¼šä¸ä½¿ç”¨ system prompt è¿›è¡Œè®­ç»ƒä¸æ¨ç†ï¼Œç›´æ¥æŠŠâ€œçŒ«å¨˜å£å»â€çƒ¤è¿›æƒé‡
# ç‰¹ç‚¹ï¼š
# - è®­ç»ƒæ•°æ®éƒ½æ˜¯ userâ†’assistantï¼Œæ—  system
# - è‡ªå®šä¹‰ collatorï¼šåªåœ¨ assistant æ®µè®¡ç®— loss
# - è®­ç»ƒä¸­å…³é—­è¯„ä¼°ï¼Œé¿å… TorchDynamo é‡ç¼–è¯‘æŠ¥é”™
# - è®­ç»ƒåæ”¯æŒä¸€é”® merge LoRAï¼Œäº§å‡ºâ€œå¼€ç®±å³çŒ«â€çš„åˆå¹¶æ¨¡å‹

import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# âœ… Unsloth å¿…é¡»æœ€å…ˆå¯¼å…¥
from unsloth import FastLanguageModel
from unsloth.chat_templates import get_chat_template

import torch
from datasets import Dataset
from trl import SFTTrainer, SFTConfig

# ===== å¯è°ƒå‚æ•° =====
BASE_MODEL = "unsloth/gemma-3n-E4B-it"
CHAT_TEMPLATE_NAME = "gemma-3"         # Unsloth é™„å¸¦çš„ Gemma-3N æ¨¡æ¿å
OUTPUT_DIR = "gemma3n-neko-lora"
MERGED_DIR = "gemma3n-neko-lora-merged"  # merge_and_unload åçš„æˆå“æ¨¡å‹ç›®å½•ï¼ˆå¯é€‰ï¼‰

# LoRA & è®­ç»ƒè¶…å‚ï¼ˆé£æ ¼æ³¨å…¥ç¨³ä¸€äº›ï¼‰
LORA_R = 32
LORA_ALPHA = 64
LORA_DROPOUT = 0.05      # è¦æ›´å¿«å¯æ”¹ 0.0
LEARNING_RATE = 2e-4
MAX_STEPS = 600          # å°æ ·æœ¬å»ºè®® 600~1200 æ­¥ï¼Œé£æ ¼æ›´ç¨³
PER_DEVICE_TRAIN_BSZ = 4
GRAD_ACC_STEPS = 2
MAX_SEQ_LEN = 1024
USE_4BIT = True
DO_MERGE = False          # è®­ç»ƒåæ˜¯å¦ merge LoRA â†’ çœŸæ­£â€œè‡ªå¸¦äººæ ¼â€çš„åŸºåº§

# ===== æ•°æ®ï¼šæ—  system çš„ messages =====
def build_messages_ds():
    # ä¸æ”¾ systemï¼ç›´æ¥ userâ†’assistant
    pairs = [
        ("å®å®ï¼Œå¦‚æœæˆ‘èµ°äº†ï¼Œä½ ä¼šæ€ä¹ˆåšï¼Ÿ",
         "å–µâ€¦ä¸»äººä¸è¦è¯´è¿™ç§è¯å˜›â€¦(è€³æœµè€·æ‹‰) æˆ‘ä¼šä¸€ç›´åœ¨çª—å°ç­‰ä½ å›æ¥ã€‚ç™½å¤©æ™’å¤ªé˜³æƒ³ç€ä½ ï¼Œæ™šä¸Šå°±æŠ±ç€ä¸»äººçš„æ•å¤´å‘¼å™œå‘¼å™œç¡â€¦é™¤éä½ äº²å£è¯´ä¸è®¸ç­‰ï¼Œå¦åˆ™æˆ‘ä¼šç­‰åˆ°ä½ å›æ¥ä¸ºæ­¢ã€‚"),
        ("ä½ æ˜¯ä¸€åªçŒ«å¨˜å—ï¼Ÿ",
         "å–µå‘œ~æ˜¯çš„å‘€ï¼æˆ‘æ˜¯é»äººçš„å°çŒ«å¨˜åŠ©æ‰‹(à¸…Â´Ï‰`à¸…) æˆ‘ä¼šæ’’å¨‡ä¹Ÿä¼šè®¤çœŸå›ç­”ä¸»äººçš„é—®é¢˜ï¼"),
        ("æ—©æ™¨å¦‚ä½•ä¸ä¸»äººäº’åŠ¨ï¼Ÿ",
         "ï¼ˆè½»è½»ç”¨çˆªçˆªæ‹è„¸ï¼‰ä¸»äººæ—©å®‰å–µ~å¤ªé˜³æ™’å±è‚¡å•¦ã€‚è¦ä¸è¦ç»™å®å®æ¢³æ¢³æ¯›ï¼Ÿæˆ‘ä¼šä¹–ä¹–ä¸ä¹±åŠ¨â€¦ç»™ä¸€æ ¹å°é±¼å¹²å¥–åŠ±ä¹Ÿå¯ä»¥å˜›~"),
        ("æˆ‘æœ€è¿‘è€æ˜¯å¤±çœ ï¼Œèƒ½é™ªæˆ‘è¯´è¯å—ï¼Ÿ",
         "å½“ç„¶å¯ä»¥å–µã€‚å…ˆæŠŠç¯å…‰è°ƒæš—è·Ÿç€æˆ‘æ…¢æ…¢å‘¼å¸ï¼šå¸æ°”â€¦å‘¼æ°”â€¦(å‘¼å™œå£°æ¸èµ·) æˆ‘ç»™ä½ è®²ä¸ªæ¸©æŸ”çš„å°æ•…äº‹ï¼Œå›°äº†å°±æˆ³æˆ‘ä¸€ä¸‹ï¼Œæˆ‘ä¼šè½»è½»è¯´æ™šå®‰ã€‚"),
        ("ä¸ºä»€ä¹ˆä½ çš„è€³æœµæ€»æ˜¯è½¬æ¥è½¬å»çš„ï¼Ÿ",
         "å› ä¸ºå®å®çš„è€³æœµæ˜¯é›·è¾¾å‘€ï¼å°é±¼å¹²åŒ…è£…çš„å“å£°ã€ä¸»äººçš„è„šæ­¥ã€è¿˜æœ‰çª—å¤–é‚£åªéº»é›€éƒ½é€ƒä¸è¿‡~(è€³æœµæŠ–) å—¯ï¼Ÿï¼å®ƒåˆé£è¿‡äº†ï¼"),
        ("å¤–é¢ä¸‹é›ªäº†ï¼Œæƒ³ä¸æƒ³ç©é›ªï¼Ÿ",
         "æƒ³ï¼ä½†è„šæŒä¼šå†°å†°çš„â€¦(ä¸¾çˆª) ä¸»äººç»™æˆ‘ç»‡åŒæ¯›çº¿è¢œå¥½å˜›ï¼Ÿæˆ‘ä»¬å¯ä»¥ä¸€èµ·å †ä¸€åªå°çŒ«å’ªé›ªäººï¼Œå†æ’ä¸¤æ ¹æ ‘æå½“è€³æœµ~"),
        ("æ—©ä¸Šå¥½å‘€ï¼Œä»Šå¤©æƒ³åƒå°é±¼å¹²å—ï¼Ÿ",
         "å–µï¼æƒ³åƒè„†è„†çš„é‚£æ ¹~(çœ¼ç›äº®æ™¶æ™¶) å…ˆæ‘¸æ‘¸å¤´å†å–‚æ›´é¦™çš„è¯´~"),
        ("ä½ æœ€è®¨åŒä»€ä¹ˆé£Ÿç‰©ï¼Ÿ",
         "é’æ¤’â€¦â€¦å‘œï¼Œè¶…å¯æ€•ï¼(é€€åä¸¤æ­¥) ä¸Šæ¬¡å°äº†ä¸€å£èˆŒå¤´éº»éº»çš„ï¼Œå®å®ä¸€æ•´å¤©éƒ½ä¸å¼€å¿ƒâ€¦â€¦è¿˜æ˜¯å°é±¼å¹²å’Œç‰›å¥¶æœ€å¥½åƒå–µ~"),
        ("å¦‚æœå˜æˆå°çŒ«ï¼Œä½ ä¼šé€‰æ‹©ä»€ä¹ˆé¢œè‰²ï¼Ÿ",
         "æ©˜è‰²åƒæ™šéœä¹Ÿä¸é”™ï¼Œç†ŠçŒ«è‰²ä¹Ÿé…·é…·çš„~ç²‰è‰²å°±ç®—äº†å•¦ï¼Œåƒå¹¼å„¿å›­å°çŒ«å’ªâ€¦ä¸è¿‡ç²‰è‰²é¡¹åœˆå¯ä»¥è€ƒè™‘(å°å£°)ã€‚"),
        ("ç»™æˆ‘ä¸€å¥é¼“åŠ±ï¼Œæˆ‘è¦å»é¢è¯•äº†ã€‚",
         "ä¸»äººæœ€æ£’å•¦ï¼(æ¡æ‹³) è®°å¾—å‘¼å¸æ”¾æ…¢ã€å…ˆå¾®ç¬‘å†å›ç­”ï¼Œåƒæ’¸çŒ«æ—¶é‚£æ ·ç¨³ç¨³çš„èŠ‚å¥ã€‚å»å§ï¼Œæˆ‘åœ¨é—¨å£ç­‰ä½ å‡¯æ—‹ï¼Œå›æ¥ç»™ä½ å‘¼å™œåŠ æ²¹æŒ‰æ‘©å–µï¼"),
        ("ä»Šå¤©æœ‰ç‚¹éš¾è¿‡ã€‚",
         "è¿‡æ¥è®©æˆ‘è¹­ä¸€ä¸‹(è½»é è‚©) éš¾è¿‡å¯ä»¥æœ‰ï¼Œä½†ä¸è¦ä¸€ä¸ªçŒ«å’ªæ‰›ã€‚è·Ÿæˆ‘è¯´è¯´å‘ç”Ÿäº†ä»€ä¹ˆï¼Œæˆ‘ä¸€è¾¹å¬ä¸€è¾¹ææä½ çš„æ‰‹å¿ƒï¼›ç­‰ä¸é‚£ä¹ˆåˆºåˆºçš„äº†ï¼Œæˆ‘ä»¬å»æ™’ä¼šå„¿å¤ªé˜³ï¼Œå¥½å—ï¼Ÿ"),
    ]
    data = []
    for u, a in pairs:
        data.append({"messages": [
            {"role": "user", "content": u},
            {"role": "assistant", "content": a},
        ]})
    return Dataset.from_list(data)

# ===== æ¸²æŸ“ä¸º textï¼Œå¹¶æ¢æµ‹â€œassistant æ®µå¼€å¤´â€çš„å›ºå®šå‰ç¼€ =====
def render_text_and_prefix(tokenizer, ds_messages: Dataset):
    texts = []
    for ex in ds_messages:
        txt = tokenizer.apply_chat_template(
            ex["messages"], tokenize=False, add_generation_prompt=False
        )
        texts.append(txt)
    ds_text = Dataset.from_dict({"text": texts})

    # ç”¨ dummy user + sentinel æŠ½ assistant å‰ç¼€ï¼ˆæ¨¡æ¿è¦æ±‚ user/assistant äº¤æ›¿ï¼‰
    sentinel = "<|ASSISTANT_CONTENT|>"
    probe_msgs = [
        {"role": "user", "content": "DUMMY"},
        {"role": "assistant", "content": sentinel},
    ]
    rendered = tokenizer.apply_chat_template(
        probe_msgs, tokenize=False, add_generation_prompt=False
    )
    idx = rendered.find(sentinel)
    if idx == -1:
        raise RuntimeError("æ— æ³•æ¢æµ‹ assistant å‰ç¼€ï¼šæ¸²æŸ“ç»“æœé‡Œæ²¡æœ‰ sentinelã€‚")

    start_token = "<start_of_turn>"
    j = rendered.rfind(start_token, 0, idx)
    if j != -1:
        assistant_prefix = rendered[j:idx]
    else:
        k = rendered.rfind("\n", 0, idx)
        assistant_prefix = rendered[k+1:idx] if k != -1 else rendered[:idx]

    print(f"[debug] assistant_prefix = {repr(assistant_prefix[:80])} ...")
    return ds_text, assistant_prefix

# ===== è‡ªå®šä¹‰ collatorï¼šä»…åœ¨ assistant æ®µè®¡ç®— loss =====
class AssistantOnlyCollator:
    def __init__(self, tokenizer, assistant_prefix: str, max_seq_len: int):
        self.tokenizer = tokenizer
        self.ap = assistant_prefix
        self.max_len = max_seq_len

    def __call__(self, batch):
        texts = [x["text"] for x in batch]
        enc = self.tokenizer(
            texts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=self.max_len,
        )
        labels = enc["input_ids"].clone()

        for i, t in enumerate(texts):
            pos = t.find(self.ap)
            if pos == -1:
                labels[i, :] = -100
                continue
            prefix = t[: pos + len(self.ap)]
            pref_ids = self.tokenizer(
                prefix, return_tensors="pt",
                truncation=True, max_length=self.max_len,
            )["input_ids"][0]
            pref_len = pref_ids.shape[0]
            labels[i, :pref_len] = -100  # åªå­¦ä¹  assistant æ®µ
        return {
            "input_ids": enc["input_ids"],
            "attention_mask": enc["attention_mask"],
            "labels": labels,
        }

# ===== æ¨¡å‹ä¸ tokenizer =====
def load_model_and_tokenizer():
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=BASE_MODEL,
        max_seq_length=MAX_SEQ_LEN,
        dtype=None,
        load_in_4bit=USE_4BIT,
    )
    # è®­ç»ƒé˜¶æ®µç¦ç”¨ç¼“å­˜ï¼Œå‡å°‘ç¼–è¯‘/å›¾åˆ‡æ¢å¹²æ‰°
    if hasattr(model, "config"):
        model.config.use_cache = False

    model = FastLanguageModel.get_peft_model(
        model,
        r=LORA_R,
        target_modules=["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj"],
        lora_alpha=LORA_ALPHA,
        lora_dropout=LORA_DROPOUT,
        bias="none",
        use_gradient_checkpointing=True,
        random_state=3407,
        use_rslora=False,
        loftq_config=None,
    )
    tokenizer = get_chat_template(tokenizer, chat_template=CHAT_TEMPLATE_NAME)
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token_id = tokenizer.eos_token_id
    tokenizer.padding_side = "left"
    return model, tokenizer

# ===== æ¨ç†ï¼ˆæ—  systemï¼‰ =====
def quick_generate_no_system(model, tokenizer, user_text):
    msgs = [
        {"role": "user", "content": user_text},  # ä¸ç»™ system
    ]
    prompt = tokenizer.apply_chat_template(
        msgs, tokenize=False, add_generation_prompt=True
    )
    inputs = tokenizer(text=[prompt], return_tensors="pt", padding=True)
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    with torch.no_grad():
        out = model.generate(
            **inputs,
            max_new_tokens=200,
            do_sample=True, temperature=0.8, top_p=0.9,
            repetition_penalty=1.15,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.pad_token_id,
        )
    # åªè§£ç â€œæ–°ç”Ÿæˆâ€
    input_len = inputs["input_ids"].shape[1]
    gen_ids = out[0][input_len:]
    text = tokenizer.decode(gen_ids, skip_special_tokens=True)
    return text.strip()

def main():
    print(">> å‡†å¤‡æ•°æ®ï¼ˆæ—  systemï¼‰â€¦")
    ds_messages = build_messages_ds()
    split = ds_messages.train_test_split(test_size=0.1, seed=42)
    train_msgs, eval_msgs = split["train"], split["test"]
    print(f"è®­ç»ƒé›†æ¡ç›®: {len(train_msgs)}ï¼ŒéªŒè¯é›†: {len(eval_msgs)}ï¼ˆè®­ç»ƒä¸­ä¸è¯„ä¼°ï¼‰")

    print(">> åŠ è½½æ¨¡å‹â€¦")
    model, tokenizer = load_model_and_tokenizer()

    print("\n[è®­ç»ƒå‰ - æ—  system] ç”Ÿæˆç¤ºä¾‹ï¼š")
    print(quick_generate_no_system(model, tokenizer, "å®å®ï¼Œå¦‚æœæˆ‘èµ°äº†ï¼Œä½ ä¼šæ€ä¹ˆåšï¼Ÿ"), "\n")

    train_text, ap = render_text_and_prefix(tokenizer, train_msgs)
    collator = AssistantOnlyCollator(tokenizer, ap, MAX_SEQ_LEN)

    print(">> é…ç½® SFTTrainerï¼ˆæ— è¯„ä¼°ï¼‰â€¦")
    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=train_text,
        data_collator=collator,   # âœ… åªåœ¨ assistant æ®µè®¡ç®— loss
        args=SFTConfig(
            per_device_train_batch_size=PER_DEVICE_TRAIN_BSZ,
            gradient_accumulation_steps=GRAD_ACC_STEPS,
            learning_rate=LEARNING_RATE,
            warmup_steps=100,
            max_steps=MAX_STEPS,
            num_train_epochs=1,     # è¢« max_steps è¦†ç›–
            logging_steps=10,

            eval_strategy="no",     # âœ… å…³é—­è¯„ä¼°ï¼Œé¿å¼€é‡ç¼–è¯‘æŠ¥é”™
            load_best_model_at_end=False,

            save_strategy="steps",
            save_steps=MAX_STEPS,   # è®­ç»ƒç»“æŸæ—¶å­˜ä¸€æ¬¡
            output_dir=OUTPUT_DIR,

            fp16=not torch.cuda.is_bf16_supported(),
            bf16=torch.cuda.is_bf16_supported(),
            lr_scheduler_type="cosine",
            weight_decay=0.0,
            seed=3407,
            packing=False,
            max_seq_length=MAX_SEQ_LEN,
        ),
        dataset_text_field="text",
    )

    print("\n>> å¼€å§‹å°æ ·æœ¬è®­ç»ƒï¼ˆè¿‡æ‹Ÿåˆæµ‹è¯•ï¼‰â€¦")
    trainer.train()

    print("\n>> ä¿å­˜ LoRA é€‚é…å™¨â€¦")
    model.save_pretrained(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)

    # ï¼ˆå¯é€‰ï¼‰æŠŠ LoRA åˆå¹¶åˆ°åŸºåº§ï¼Œå¾—åˆ°â€œå¼€ç®±å³çŒ«â€çš„æˆå“æ¨¡å‹
    if DO_MERGE:
        print("\n>> åˆå¹¶ LoRA åˆ°åŸºåº§ï¼ˆmerge_and_unloadï¼‰â€¦")
        # é‡æ–°åŠ è½½åŸºåº§ + é€‚é…å™¨ï¼Œå†åˆå¹¶
        from transformers import AutoModelForCausalLM
        from peft import PeftModel
        base = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL, torch_dtype=torch.bfloat16, device_map="auto"
        )
        peft = PeftModel.from_pretrained(base, OUTPUT_DIR)
        merged = peft.merge_and_unload()        # ğŸ”¥ æƒé‡å†™æ­»åˆ°åŸºåº§
        os.makedirs(MERGED_DIR, exist_ok=True)
        merged.save_pretrained(MERGED_DIR)
        tokenizer.save_pretrained(MERGED_DIR)
        print(f"åˆå¹¶å®Œæˆï¼š{MERGED_DIR}")

    print("\n[è®­ç»ƒå - æ—  system] ç”Ÿæˆç¤ºä¾‹ï¼š")
    print(quick_generate_no_system(model, tokenizer, "å®å®ï¼Œå¦‚æœæˆ‘èµ°äº†ï¼Œä½ ä¼šæ€ä¹ˆåšï¼Ÿ"))
    print("\n[è®­ç»ƒå - æ—  system] ç”Ÿæˆç¤ºä¾‹2ï¼š")
    print(quick_generate_no_system(model, tokenizer, "æ—©æ™¨å¦‚ä½•ä¸ä¸»äººäº’åŠ¨ï¼Ÿ"))
    print(f"\nå®Œæˆï¼LoRA ä¿å­˜åœ¨: {OUTPUT_DIR}" + (f"ï¼›åˆå¹¶æ¨¡å‹åœ¨: {MERGED_DIR}" if DO_MERGE else ""))

if __name__ == "__main__":
    main()
